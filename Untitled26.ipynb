{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsYMbEX4N1X-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f =open(\"chatbot.txt\", \"r\", errors='ignore') # opens a file \n",
        "raw_doc = f.read() # reads the contents of the file \n",
        "raw_doc = raw_doc.lower() # lowercase\n",
        "nltk.download(\"punkt\") # The punkt module provides data for NLTK's pre-trained sentence tokenizer.\n",
        "nltk.download(\"wordnet\") # WordNet is a lexical database for English\n",
        "sent_tokens = nltk.sent_tokenize(raw_doc)\n",
        "word_tokens = nltk.word_tokenize(raw_doc)\n"
      ],
      "metadata": {
        "id": "gAiPc53QN74P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b939e4d1-5b0b-4520-bf7a-d1ef0fd5fc04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "checking the word_tokens"
      ],
      "metadata": {
        "id": "DnYRF5POYEiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens[:2]"
      ],
      "metadata": {
        "id": "LB236GP3X7GF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ea2943-c237-4b55-dfaa-92e16a52ed9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['binita', 'thapa']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "text preprocessing"
      ],
      "metadata": {
        "id": "WzZ6ZXhdYIyF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qys_53l2X7Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "#takes a list of tokens as input and returns a new list where each token has been lemmatized\n",
        "#creates a dictionary where each punctuation character is a key and the corresponding value is None. \n",
        "#This is used to remove punctuation from the input text by mapping each punctuation character to None    \n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def lemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "def LemNormalize(text):\n",
        "  return lemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
        "  "
      ],
      "metadata": {
        "id": "ut6nty36X7In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GREET_INPUTS =(\"hello\", \"hi\", \"greetings\", \"sup\", \"whatsapp\", \"hey\")\n",
        "GREET_RESPONSE =[\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad\", \"You are talking to me\"]\n",
        "\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in GREET_INPUTS:\n",
        "      return random.choice(GREET_RESPONSE)"
      ],
      "metadata": {
        "id": "KMALtejfimBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response Generation"
      ],
      "metadata": {
        "id": "qzJhdPXjsa2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n"
      ],
      "metadata": {
        "id": "HEIrJ2nzsY9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b77ff77-8396-439c-94c4-1635797d1083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_response):\n",
        "    robo1_response = \" \"\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=\"english\")\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)#transform it into a matrix of TF-IDF features.\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)#resulting vals matrix contains the similarity scores \n",
        "                                              #between the last sentence and all the other sentences in the list.\n",
        "    idx = vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if req_tfidf == 0:\n",
        "        robo1_response = robo1_response + \"I'm sorry, I don't understand you\"\n",
        "        return robo1_response\n",
        "    else:\n",
        "        robo1_response = robo1_response + sent_tokens[idx]\n",
        "        return robo1_response\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D7KjojBitbY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PqJS4DMZsw_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining conservations start/end protocols"
      ],
      "metadata": {
        "id": "hgDrBNgProcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flag = True\n",
        "print(\"BOT: My name is Hary, Let's start a conversation! If you want to exit, just say Bye!\")\n",
        "\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response = user_response.lower()\n",
        "    if(user_response!=\"bye\"):\n",
        "        if(user_response==\"thanks\" or user_response==\"thank you\"):\n",
        "            flag=False\n",
        "            print(\"BOT: You are welcome!\")\n",
        "        else:\n",
        "            if(greet(user_response)!=None):\n",
        "                print(\"BOT: \"+greet(user_response))\n",
        "            else:\n",
        "                sent_tokens.append(user_response)\n",
        "                word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
        "                final_words=list(set(word_tokens))\n",
        "                print(\"BOT: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                # Remove the sent token that was just used for the response to avoid repetition\n",
        "                if user_response in sent_tokens:\n",
        "                    sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"BOT: Bye! take care..\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6pTshrO7rz3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef02170-6ead-4d61-bcc4-8c19b382e9e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT: My name is Hary, Let's start a conversation! If you want to exit, just say Bye!\n",
            "BOT: I am glad\n",
            "BOT:  \n",
            "binita thapa is working as a research specialist in prarieview a&m \n",
            "microbiology (from ancient greek μῑκρος (mīkros) 'small', βίος (bíos) 'life', and -λογία (-logía) 'study of') is the scientific study of microorganisms, those being unicellular (single cell), multicellular (cell colony), or acellular (lacking cells).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ImQiTTkyj89x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}